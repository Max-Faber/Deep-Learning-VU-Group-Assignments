{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ_pmgxvGur9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 4b - Graph Convolutional Networks\n",
    "## Deep Learning Course - Vrije Universiteit Amsterdam, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEneMITS2agU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Instructions on how to use this notebook:\n",
    "\n",
    "This notebook is hosted on Google Colab. To be able to work on it, you have to create your own copy. Go to *File* and select *Save a copy in Drive*.\n",
    "\n",
    "You can also avoid using Colab entirely, and download the notebook to run it on your own machine. If you choose this, go to *File* and select *Download .ipynb*.\n",
    "\n",
    "The advantage of using Colab is that you can use a GPU. You can complete this assignment with a CPU, but it will take a bit longer. Furthermore, we encourage you to train using the GPU not only for faster training, but also to get experience with this setting. This includes moving models and tensors to the GPU and back. This experience is very valuable because for many interesting models and large datasets (like large CNNs for ImageNet, or Transformer models trained on Wikipedia), training on GPU is the only feasible way.\n",
    "\n",
    "The default Colab runtime does not have a GPU. To change this, go to *Runtime - Change runtime type*, and select *GPU* as the hardware accelerator. The GPU that you get changes according to what resources are available at the time, and its memory can go from a 5GB, to around 18GB if you are lucky. If you are curious, you can run the following in a code cell to check:\n",
    "\n",
    "```sh\n",
    "!nvidia-smi\n",
    "```\n",
    "\n",
    "Note that despite the name, Google Colab does  not support collaborative work without issues. When two or more people edit the notebook concurrently, only one version will be saved. You can choose to do group programming with one person sharing the screen with the others, or make multiple copies of the notebook to work concurrently.\n",
    "\n",
    "**Submission:** Upload your notebook in .ipynb format to Canvas. The code and answers to the questions in the notebook are sufficient, no separate report is expected. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lBgoJIpdLI2Y",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2f4f1d93-2f77-4ed1-ab7d-ab814daab38e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import scipy.sparse\n",
    "!nvidia-smi"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsdc7fDp40rQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Introduction\n",
    "\n",
    "Graphs are very useful data structures that allow us to represent sets of entities and the way they are related among each other. In a graph, entities are also known as *nodes*, and any link between entities is also called an *edge*.\n",
    "\n",
    "Examples of real world objects that can be modeled as graphs are social networks, where entities are people and relations denote friendship; and molecules, where entities are atoms and relations indicate a bond between them.\n",
    "\n",
    "There has been increased interest in the recent years in the application of deep learning architectures to graph-structured data, for tasks like predicting missing relations between entities, classifying entities, and classifying graphs. This interest has been spurred by the introduction of Graph Convolutional Networks (GCNs).\n",
    "\n",
    "In this assignment, you will implement and experiment with one of the first versions of the GCN, proposed by Thomas Kipf and Max Welling in their 2017 paper, [Semi-supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907). In particular, the goals of this assignment are to\n",
    "\n",
    "- Understand how GCNs are formulated\n",
    "- Implement the GCN using PyTorch\n",
    "- Train and evaluate a model for semi-supervised node classification in citation networks\n",
    "- Train and evaluate a model for binary classification of molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvsuVNczG6pP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Representing graphs\n",
    "\n",
    "Suppose we have the following graph:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/01-graph.png\" width=\"200\">\n",
    "\n",
    "This is an undirected graph (since the edges have no specified direction) with 4 nodes. One way to represent the connectivity structure of the graph is by means of the **adjacency matrix**. The $i$-th row of the matrix contains a 1 in the $j$-th column, if nodes $i$ and $j$ are connected. For an undirected graph like the one above, this means that the adjacency matrix\n",
    "\n",
    "- Is symmetric (e.g. an edge between 0 and 2 is equivalent as an edge between 2 and 0)\n",
    "- Is square, of size $n\\times n$ where $n$ is the number of nodes\n",
    "\n",
    "The adjacency matrix for the graph above is then the following:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 1 & 0 \\\\ \n",
    "0 & 0 & 1 & 0 \\\\\n",
    "1 & 1 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "A second matrix of interest is the **degree matrix**. This is a diagonal matrix where the $i$-th element of the diagonal indicates the number of edges connected to node $i$. Note that these can be obtained from $A$ by summing across the columns, or the rows. For our example, the degree matrix is\n",
    "\n",
    "$$\n",
    "D = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\ \n",
    "0 & 1 & 0 & 0 \\\\\n",
    "0 & 0 & 3 & 0 \\\\\n",
    "0 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For specific applications, each node in the graph will have an associated vector of features $x\\in\\mathbb{R}^c$. If our graph is a social network, then the vector of features can contain information like age, location, and musical tastes, in a specific numeric format. In the case of a molecule, the node could represent an atom and have features like the atomic mass, etc. We can lay out the features in a matrix $X\\in\\mathbb{R}^{n\\times c}$, so that the feature vector for node $i$ is in the $i$-th row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCEQ2ffzHCf2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading a citation network\n",
    "\n",
    "To move to a real world example, we will start with the Cora dataset. This dataset represents a citation network, where nodes are scientific publications, edges denote citations between them, and features are a [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) extracted from their contents.\n",
    "\n",
    "This graph contains labels for nodes, that represent a specific topic. We will use these for a node classification task.\n",
    "\n",
    "To easily load it, we will use [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/index.html) (PyG), a deep learning library for graph-structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch-scatter in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (2.1.0)\r\n",
      "Requirement already satisfied: torch-cluster in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (1.6.0)\r\n",
      "Requirement already satisfied: torch-spline-conv in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (1.2.1)\r\n",
      "Requirement already satisfied: torch-geometric in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (2.2.0)\r\n",
      "Requirement already satisfied: pyparsing in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (from torch-geometric) (3.0.9)\r\n",
      "Requirement already satisfied: tqdm in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (from torch-geometric) (4.64.1)\r\n",
      "Requirement already satisfied: numpy in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (from torch-geometric) (1.23.5)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (from torch-geometric) (5.9.4)\r\n",
      "Requirement already satisfied: requests in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (from torch-geometric) (2.28.1)\r\n",
      "Requirement already satisfied: jinja2 in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (from torch-geometric) (3.1.2)\r\n",
      "Requirement already satisfied: scipy in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (from torch-geometric) (1.9.3)\r\n",
      "Requirement already satisfied: scikit-learn in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (from torch-geometric) (1.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (from jinja2->torch-geometric) (2.1.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (from requests->torch-geometric) (1.26.13)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (from requests->torch-geometric) (2022.9.24)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (from requests->torch-geometric) (2.1.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (from requests->torch-geometric) (3.4)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (3.1.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (from scikit-learn->torch-geometric) (1.2.0)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\r\n",
      "You should consider upgrading via the '/Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n",
      "Collecting git+https://github.com/rusty1s/pytorch_sparse.git\r\n",
      "  Cloning https://github.com/rusty1s/pytorch_sparse.git to /private/var/folders/nm/cgzggg4x2g79c5hxc8vwn8tm0000gn/T/pip-req-build-iurzs9pp\r\n",
      "  Running command git clone --filter=blob:none -q https://github.com/rusty1s/pytorch_sparse.git /private/var/folders/nm/cgzggg4x2g79c5hxc8vwn8tm0000gn/T/pip-req-build-iurzs9pp\r\n",
      "  Resolved https://github.com/rusty1s/pytorch_sparse.git to commit 955b1cf31ac0e12027dbf3008770e494ac5efd9c\r\n",
      "  Running command git submodule update --init --recursive -q\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: scipy in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (from torch-sparse==0.6.15) (1.9.3)\r\n",
      "Requirement already satisfied: numpy<1.26.0,>=1.18.5 in /Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages (from scipy->torch-sparse==0.6.15) (1.23.5)\r\n",
      "\u001B[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\r\n",
      "You should consider upgrading via the '/Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch Geometric\n",
    "!pip install torch-scatter torch-cluster torch-spline-conv torch-geometric #-f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
    "!pip install git+https://github.com/rusty1s/pytorch_sparse.git"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now use the library to download and import the dataset. Initializing the `Planetoid` class returns a `Dataset` object that can contain multiple graphs. In this task we will only use the `Cora` dataset (the citation network) and hence, we will select only the first element."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "data = Planetoid(root='data/Planetoid', name='Cora')[0]\n",
    "# data = data.to('mps')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### Question 1 (0.25 pt)\n",
    "\n",
    "The `data` object is an instance of the `Data` class in PyG. Check the [documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html) and report the following properties of the graph:\n",
    "\n",
    "- Number of nodes\n",
    "- Number of edges \n",
    "- The dimension $c$ of the feature vectors $x\\in\\mathbb{R}^c$\n",
    "- The number of targets for the classification task"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Dimension c of the feature vectors: 1433\n",
      "Number of targets for the classification task: 7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Your answer here\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Dimension c of the feature vectors: {data.num_node_features}')\n",
    "n_classes = len(np.unique(data.y))\n",
    "print(f'Number of targets for the classification task: {n_classes}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Question 2 (0.25 pt)\n",
    "\n",
    "In PyG, edges are provided in a tensor of shape (2, number of edges). You can access it via `data.edge_index`. Each column in this tensor contains the IDs for two nodes that are connected in the graph.\n",
    "\n",
    "We saw that in an undirected graph, an edge between nodes $i$ and $j$ adds a value of 1 to positions $(i, j)$ and $(j, i)$ of the adjacency matrix. Is this also true for the edge index? That is, if there is an edge $(i, j)$ in `data.edge_index`, is there also an edge for $(j, i)$? This is important to know for the next steps of the implementation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An edge between nodes i and j adds a value of 1 to positions (i, j) and (j, i) of the adjacency matrix.\n",
      "This is not also true for the edge index\n",
      "Because, the number of edges (10556) is also mentioned in the shape of data.edge_index: torch.Size([2, 10556])\n",
      "The edge_index property distinguishes the direction of the edge, for example it is possible that there's an edge from node 5 to node 2 but not from node 2 to node 5.\n",
      "In this case, the number of edges in the data is equal to the number of edges in edge_index. This indicates that for an edge i,j it doesn't hold that there is an edge j,i.\n"
     ]
    }
   ],
   "source": [
    "# Your answer here\n",
    "print('An edge between nodes i and j adds a value of 1 to positions (i, j) and (j, i) of the adjacency matrix.')\n",
    "print('This is not also true for the edge index')\n",
    "print(f'Because, the number of edges ({data.num_edges}) is also mentioned in the shape of data.edge_index: {data.edge_index.shape}')\n",
    "print(f'The edge_index property distinguishes the direction of the edge, for example it is possible that there\\'s an edge from node 5 to node 2 but not from node 2 to node 5.')\n",
    "print(\"In this case, the number of edges in the data is equal to the number of edges in edge_index. This indicates that for an edge i,j it doesn't hold that there is an edge j,i.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Question 3 (0.5 pt)\n",
    "\n",
    "In graphs, especially large ones, the adjacency matrix is **sparse**: most entries are zero. Sparse matrices allow for efficient storage and computation.\n",
    "\n",
    "To prepare and pre-process sparse matrices, we will use [`scipy.sparse`](https://docs.scipy.org/doc/scipy/reference/sparse.html). Once the matrices are ready, we will convert them to PyTorch tensors.\n",
    "\n",
    "We will use the [Sparse COO format](https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO)). We encourage you to first get familiar with how it works after continuing with the assignment.\n",
    "\n",
    "- Use the [`scipy.sparse.coo_matrix()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html) function to build the adjacency matrix. Think of what arguments are needed, and how you can obtain them from the graph data loaded above.\n",
    "- Use the `sum()` method of sparse matrices, together with `scipy.sparse.diags()`, to compute the degree matrix using the definition above.\n",
    "\n",
    "Both resulting matrices must be sparse of type `float32`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QC01OjbJs92-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Your answer here\n",
    "from scipy.sparse import coo_matrix, diags\n",
    "import numpy as np\n",
    "\n",
    "# The rows and columns can be extracted from the edge_index of the dataset\n",
    "row = data.edge_index[0]\n",
    "col = data.edge_index[1]\n",
    "num_edges = data.edge_index.shape[1]\n",
    "# All the indices in the row and col vector are connected with each other, therefore we can generate a 1 (connection) for all of them\n",
    "d = np.ones(num_edges, dtype=np.float32)\n",
    "coo_mat = coo_matrix((d, (row, col)), shape=(data.num_nodes, data.num_nodes), dtype=np.float32)\n",
    "print(f'Adjacency matrix of the dataset generated with the coo_matrix() function:\\n{coo_mat.toarray()}')\n",
    "\n",
    "print()\n",
    "\n",
    "# Use the sum() method to get the number of edges for each row in the adjacency matrix\n",
    "diagonal = coo_mat.sum(axis=0, dtype=np.float32).A1\n",
    "# Use the diags() method to construct the corresponding degree matrix from the diagonal computed above\n",
    "diags_mat = diags(diagonal, dtype=np.float32)\n",
    "print(f'Degree matrix of the dataset generated with the diags() function:\\n{diags_mat.toarray()}')"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix of the dataset generated with the coo_matrix() function:\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n",
      "\n",
      "Degree matrix of the dataset generated with the diags() function:\n",
      "[[3. 0. 0. ... 0. 0. 0.]\n",
      " [0. 3. 0. ... 0. 0. 0.]\n",
      " [0. 0. 5. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 4. 0.]\n",
      " [0. 0. 0. ... 0. 0. 4.]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIEJyQi2TzyY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You might wonder why we suggest to use a scipy sparse matrix, while also PyTorch supports them. The reason is that in the next step, we will be multiplying two sparse matrices, an operation not supported in PyTorch. PyTorch only allows multiplying a sparse matrix with a dense one, something which we will be doing at a later stage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlmzSb0up4LB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The Graph Convolutional Network\n",
    "\n",
    "The goal of the graph convolution is to take the feature vectors of all nodes $X\\in\\mathbb{R}^{n\\times c}$, and propagate them along the existing edges, to obtain updated representations $Z\\in\\mathbb{R}^{n\\times d}$.\n",
    "\n",
    "\n",
    "The GCN is initially motivated as performing a convolution, similarly as it is done in CNNs for images, for graph-structured data. In Kipf and Welling (2017), a theoretical derivation leads to the following formula:\n",
    "\n",
    "$$\n",
    "Z = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}XW\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $W\\in\\mathbb{R}^{c\\times d}$ is a matrix of parameters to be learned via gradient descent\n",
    "- $\\tilde{A} = A + I_n$, where $I_n$ is an $n\\times n$ identity matrix\n",
    "- $\\tilde{D}$ is the degree matrix computed with $\\tilde{A}$ as the adjacency matrix\n",
    "\n",
    "If we define $\\hat{A} = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}$, the graph convolution can be written as $Z = \\hat{A}XW$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LL4b-MTvysBp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Question 4 (0.25 pt)\n",
    "Given the formula for the GCN, explain why it operates by propagating feature vectors across the graph. To answer this, it might be useful to recall the definitions of the adjacency and degree matrices, and how they are involved in the formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vgx2SkTTyiSN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Your answer here*\n",
    "The GCN operates by propagating feature vectors across the graph. We will try to explain this by elaborating on the different variables in formula $Z$:\n",
    "\n",
    "$\\tilde{A}$ is constructed by adding up the identity matrix to the adjacency matrix. This makes sure that every node is connected to itself.\n",
    "Using $\\tilde{A}$, $\\tilde{D}$ (the degree matrix) is constructed, as we did in question 3. $\\tilde{D}$ represents the weight of every node as in the no. edges it has. However, the power of this matrix is reduced by normalizing it (using the power of $-\\frac{1}{2}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUGABEqxylsd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Question 5 (0.5 pt)\n",
    "\n",
    "Compute the **normalized adjacency matrix** $\\hat{A}$. The result should be a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GPZbnSaSyDzO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from scipy.sparse import identity\n",
    "\n",
    "a = coo_mat\n",
    "a_tilde = a + identity(a.shape[0], dtype=np.float32)\n",
    "diagonal = a_tilde.sum(axis=0, dtype=np.float32).A1\n",
    "d_tilde = diags(diagonal, dtype=np.float32)\n",
    "a_hat = d_tilde.power(-.5) @ a_tilde @ d_tilde.power(-.5)\n",
    "a_hat = a_hat.toarray()"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLLdGdZoMEy-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Question 6 (0.5 pt)\n",
    "\n",
    "So far we have used scipy to build and compute sparse matrices. Since we want to train a GCN with PyTorch, we need to convert $\\hat{A}$ into a sparse PyTorch tensor. You can do this with the [`torch.sparse_coo_tensor()`](https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html) function, making sure to specify `torch.float` as the type."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dgDsVHzEM32F",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from torch import sparse_coo_tensor\n",
    "import torch\n",
    "\n",
    "# Your answer here\n",
    "# The indices can be constructed using np.arange(), as the matrix a_hat only contains non-zero values on the diagonal\n",
    "indices = torch.tensor([np.arange(data.num_nodes), np.arange(data.num_nodes)])\n",
    "# Extract the values that correspond to the indices\n",
    "values = torch.tensor(np.array(np.diagonal(a_hat)))\n",
    "# Now use the indices and values to construct the sparse COO tensor\n",
    "sparse_coo = sparse_coo_tensor(indices, values, dtype=torch.float)\n",
    "sparse_coo\n"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nm/cgzggg4x2g79c5hxc8vwn8tm0000gn/T/ipykernel_3112/1525742703.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  indices = torch.tensor([np.arange(data.num_nodes), np.arange(data.num_nodes)])\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor(indices=tensor([[   0,    1,    2,  ..., 2705, 2706, 2707],\n                       [   0,    1,    2,  ..., 2705, 2706, 2707]]),\n       values=tensor([0.2500, 0.2500, 0.1667,  ..., 0.5000, 0.2000, 0.2000]),\n       size=(2708, 2708), nnz=2708, layout=torch.sparse_coo)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAlRVT5aODkX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Question 7 (0.5 pt)\n",
    "\n",
    "We now have all the ingredients to build a GCN layer. Implement a class (inheriting from `torch.nn.Module`) with a learnable matrix of weights $W\\in\\mathbb{R}^{c\\times d}$. Make sure to\n",
    "\n",
    "- Call this class `GCNLayer`\n",
    "- The `__init__()` constructor should take as argument the number of input and output features.\n",
    "- Use `torch.nn.init.kaiming_uniform_` to initialize $W$.\n",
    "- Define the `forward` method, which takes as input $X$ and $\\hat{A}$ and returns $Z$. Note that multiplications involving the sparse matrix $\\hat{A}$ have to be done with `torch.spmm`. \n",
    "\n",
    "Once you have implemented the class, instantiate a layer with the correct number of input features for the Cora dataset, and a number of output features of your choice. Do a forward pass and report the shape of the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JFCohhhwPpTT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Your answer here\n",
    "class GCNLayer(torch.nn.Module):\n",
    "    def __init__(self, n_input_features, n_output_features):\n",
    "        super().__init__()\n",
    "        self.W = torch.nn.parameter.Parameter(torch.empty(n_input_features, n_output_features))\n",
    "        torch.nn.init.kaiming_uniform_(self.W, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, X, A_hat):\n",
    "        Z = torch.spmm(torch.spmm(X.T, A_hat).T, self.W)\n",
    "        return Z\n",
    "\n",
    "gcn_layer = GCNLayer(data.num_features, n_classes)\n",
    "output_tensor = gcn_layer.forward(data.x, torch.tensor(a_hat))\n",
    "print(f'Shape of the output tensor: {output_tensor.shape}')"
   ],
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the output tensor: torch.Size([2708, 7])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ptAiizZUKaM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Question 8 (0.5 pt)\n",
    "\n",
    "As we have seen so far, the GCN layer implements a special type of linear transformation of the inputs. However, it is often beneficial in deep learning to stack multiple, non-linear transformations of the input features. Implement a second module class for a model with two GCN layers (use the module you implemented in the previous question).\n",
    "\n",
    "- Call this class `GCN`\n",
    "- The constructor must now take as input the number of input features, the output dimension of the first layer (this is the hidden layer), and the output dimension of the output layer.\n",
    "- In the forward pass, add a ReLU activation function after the first layer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2zhyu3S9Vj3b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import torch_geometric\n",
    "\n",
    "# Your answer here\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, n_input_features, out_dim_hid, out_dim_out):\n",
    "        super().__init__()\n",
    "        self.hidden = GCNLayer(n_input_features=n_input_features, n_output_features=out_dim_hid)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.output = GCNLayer(n_input_features=out_dim_hid, n_output_features=out_dim_out)\n",
    "        # self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, X, A_hat):\n",
    "        X = self.hidden(X, A_hat)\n",
    "        X = self.relu(X)\n",
    "        X = self.output(X, A_hat)\n",
    "        # X = self.softmax(X)\n",
    "        return X"
   ],
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NVB-3I5Wfkf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### GCNs for semi-supervised node classification\n",
    "\n",
    "Now that we have a GCN with two layers, we can test its performance in a node classification task. We will pass the input node features $X$ through the GCN layers, and the output will be of size $n\\times k$ where $k$ is the number of classes (which you found in question 1). The label denotes the topic an article in the citation network belongs to (e.g. physics, computer science, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trc4dSa7cuQj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Question 9 (1.5 pt)\n",
    "\n",
    "Note that the `data` object contains all labels (for all splits) in `data.y`, and binary masks for the train, validation, and test splits in `data.train_mask`, `data.val_mask`, and `data.test_mask`, respectively. These masks are the same size as `data.y`, and indicate which labels belong to which split.\n",
    "\n",
    "- Create a GCN with two layers (using the class from the previous question), with 32 as the hidden dimension, and the number of output features equal to the number of classes in the Cora dataset.\n",
    "\n",
    "- Use the Adam optimizer with a learning rate of 0.01.\n",
    "\n",
    "- Implement a training loop for the GCN. At each step, pass $X$ and $\\hat{A}$ to the GCN to obtain the logits. Compute the mean cross-entropy loss **only for the training instances**, using the binary masks.\n",
    "\n",
    "- After each training step, evaluate the accuracy for the validation instances.\n",
    "\n",
    "- Train for 100 epochs. Once training is finished, plot the training loss and validation accuracy (in a graph in function of the epoch number), and report the accuracy in the test set.\n",
    "\n",
    "You should obtain an accuracy over 75% on both the validation and test sets. You can also compare your results with the original paper, which also contains results for the Cora dataset. Give a brief discussion on the results of your experiments.\n",
    "\n",
    "Note that in contrast with other tasks, like image classification on some datasets, we don't use mini-batches here. The whole matrix of features and the adjacency is passed to the GCN in one step."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5Z2OP_ZRWlmo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Your answer here\n",
    "import time\n",
    "\n",
    "A_hat = torch.tensor(a_hat)\n",
    "\n",
    "gcn = GCN(n_input_features=data.num_features, out_dim_hid=32, out_dim_out=n_classes)\n",
    "optimizer = torch.optim.Adam(lr=0.01, params=gcn.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def evaluate(A_hat, data, gcn, mask):\n",
    "    gcn.eval()\n",
    "    n_instances = mask.sum().item()\n",
    "    outputs = gcn(data.x, A_hat)[mask]\n",
    "    predicted_classes = torch.argmax(outputs, dim=1)\n",
    "    n_correct = (predicted_classes == data.y[mask]).sum().item()\n",
    "    acc = n_correct / n_instances\n",
    "    return acc\n",
    "\n",
    "def training_loop(epochs, data, A_hat, optimizer, criterion, gcn):\n",
    "    train_n_instances = data.train_mask.sum()\n",
    "    results = {\n",
    "        'epoch': [],\n",
    "        'train_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    gcn.train()\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = gcn(data.x, A_hat)[data.train_mask]\n",
    "        softmax = torch.nn.Softmax(dim=1)\n",
    "        outputs = softmax(outputs)\n",
    "        predicted_classes = torch.argmax(outputs, dim=1)\n",
    "        n_correct_train = (predicted_classes == data.y[data.train_mask]).sum().item()\n",
    "        loss = criterion(outputs, data.y[data.train_mask].type(torch.long))\n",
    "        running_loss_train = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        results['epoch'].append(epoch + 1)\n",
    "        results['train_loss'].append(running_loss_train)\n",
    "        results['val_acc'].append(evaluate(A_hat=A_hat, data=data, gcn=gcn, mask=data.val_mask))\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, loss: {(running_loss_train):.3f}, accuracy: {(n_correct_train / train_n_instances):.3f}, time: {(time.time() - start):2f} seconds')\n",
    "    return gcn, results\n",
    "\n",
    "epochs = 100\n",
    "gcn, results = training_loop(\n",
    "    epochs=epochs,\n",
    "    data=data,\n",
    "    A_hat=A_hat,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    gcn=gcn)\n",
    "test_acc = evaluate(A_hat=A_hat, data=data, gcn=gcn, mask=data.test_mask)"
   ],
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, loss: 1.943, accuracy: 0.164, time: 0.043864 seconds\n",
      "Epoch 2/100, loss: 1.872, accuracy: 0.343, time: 0.039564 seconds\n",
      "Epoch 3/100, loss: 1.795, accuracy: 0.471, time: 0.039279 seconds\n",
      "Epoch 4/100, loss: 1.717, accuracy: 0.550, time: 0.037741 seconds\n",
      "Epoch 5/100, loss: 1.640, accuracy: 0.671, time: 0.037817 seconds\n",
      "Epoch 6/100, loss: 1.566, accuracy: 0.793, time: 0.039096 seconds\n",
      "Epoch 7/100, loss: 1.498, accuracy: 0.864, time: 0.041354 seconds\n",
      "Epoch 8/100, loss: 1.436, accuracy: 0.900, time: 0.041695 seconds\n",
      "Epoch 9/100, loss: 1.382, accuracy: 0.936, time: 0.041920 seconds\n",
      "Epoch 10/100, loss: 1.335, accuracy: 0.950, time: 0.045579 seconds\n",
      "Epoch 11/100, loss: 1.296, accuracy: 0.950, time: 0.041524 seconds\n",
      "Epoch 12/100, loss: 1.266, accuracy: 0.957, time: 0.038573 seconds\n",
      "Epoch 13/100, loss: 1.243, accuracy: 0.964, time: 0.043819 seconds\n",
      "Epoch 14/100, loss: 1.224, accuracy: 0.971, time: 0.040719 seconds\n",
      "Epoch 15/100, loss: 1.209, accuracy: 0.979, time: 0.038087 seconds\n",
      "Epoch 16/100, loss: 1.198, accuracy: 0.993, time: 0.040870 seconds\n",
      "Epoch 17/100, loss: 1.190, accuracy: 0.993, time: 0.038503 seconds\n",
      "Epoch 18/100, loss: 1.185, accuracy: 0.993, time: 0.039091 seconds\n",
      "Epoch 19/100, loss: 1.181, accuracy: 0.993, time: 0.041176 seconds\n",
      "Epoch 20/100, loss: 1.178, accuracy: 0.993, time: 0.041795 seconds\n",
      "Epoch 21/100, loss: 1.176, accuracy: 0.993, time: 0.044456 seconds\n",
      "Epoch 22/100, loss: 1.174, accuracy: 0.993, time: 0.041351 seconds\n",
      "Epoch 23/100, loss: 1.171, accuracy: 1.000, time: 0.040530 seconds\n",
      "Epoch 24/100, loss: 1.169, accuracy: 1.000, time: 0.043054 seconds\n",
      "Epoch 25/100, loss: 1.168, accuracy: 1.000, time: 0.038040 seconds\n",
      "Epoch 26/100, loss: 1.168, accuracy: 1.000, time: 0.037869 seconds\n",
      "Epoch 27/100, loss: 1.168, accuracy: 1.000, time: 0.038612 seconds\n",
      "Epoch 28/100, loss: 1.167, accuracy: 1.000, time: 0.040053 seconds\n",
      "Epoch 29/100, loss: 1.167, accuracy: 1.000, time: 0.038366 seconds\n",
      "Epoch 30/100, loss: 1.167, accuracy: 1.000, time: 0.038189 seconds\n",
      "Epoch 31/100, loss: 1.166, accuracy: 1.000, time: 0.038297 seconds\n",
      "Epoch 32/100, loss: 1.166, accuracy: 1.000, time: 0.041563 seconds\n",
      "Epoch 33/100, loss: 1.166, accuracy: 1.000, time: 0.042403 seconds\n",
      "Epoch 34/100, loss: 1.166, accuracy: 1.000, time: 0.043352 seconds\n",
      "Epoch 35/100, loss: 1.166, accuracy: 1.000, time: 0.048761 seconds\n",
      "Epoch 36/100, loss: 1.166, accuracy: 1.000, time: 0.042299 seconds\n",
      "Epoch 37/100, loss: 1.166, accuracy: 1.000, time: 0.038428 seconds\n",
      "Epoch 38/100, loss: 1.166, accuracy: 1.000, time: 0.038749 seconds\n",
      "Epoch 39/100, loss: 1.166, accuracy: 1.000, time: 0.038524 seconds\n",
      "Epoch 40/100, loss: 1.166, accuracy: 1.000, time: 0.037900 seconds\n",
      "Epoch 41/100, loss: 1.166, accuracy: 1.000, time: 0.038847 seconds\n",
      "Epoch 42/100, loss: 1.166, accuracy: 1.000, time: 0.038080 seconds\n",
      "Epoch 43/100, loss: 1.166, accuracy: 1.000, time: 0.038360 seconds\n",
      "Epoch 44/100, loss: 1.166, accuracy: 1.000, time: 0.038884 seconds\n",
      "Epoch 45/100, loss: 1.166, accuracy: 1.000, time: 0.039336 seconds\n",
      "Epoch 46/100, loss: 1.166, accuracy: 1.000, time: 0.040997 seconds\n",
      "Epoch 47/100, loss: 1.166, accuracy: 1.000, time: 0.040414 seconds\n",
      "Epoch 48/100, loss: 1.166, accuracy: 1.000, time: 0.040647 seconds\n",
      "Epoch 49/100, loss: 1.166, accuracy: 1.000, time: 0.044346 seconds\n",
      "Epoch 50/100, loss: 1.166, accuracy: 1.000, time: 0.043322 seconds\n",
      "Epoch 51/100, loss: 1.166, accuracy: 1.000, time: 0.042252 seconds\n",
      "Epoch 52/100, loss: 1.166, accuracy: 1.000, time: 0.038978 seconds\n",
      "Epoch 53/100, loss: 1.166, accuracy: 1.000, time: 0.041841 seconds\n",
      "Epoch 54/100, loss: 1.166, accuracy: 1.000, time: 0.039575 seconds\n",
      "Epoch 55/100, loss: 1.166, accuracy: 1.000, time: 0.038340 seconds\n",
      "Epoch 56/100, loss: 1.166, accuracy: 1.000, time: 0.041721 seconds\n",
      "Epoch 57/100, loss: 1.166, accuracy: 1.000, time: 0.042881 seconds\n",
      "Epoch 58/100, loss: 1.166, accuracy: 1.000, time: 0.040928 seconds\n",
      "Epoch 59/100, loss: 1.166, accuracy: 1.000, time: 0.042000 seconds\n",
      "Epoch 60/100, loss: 1.166, accuracy: 1.000, time: 0.038341 seconds\n",
      "Epoch 61/100, loss: 1.166, accuracy: 1.000, time: 0.037926 seconds\n",
      "Epoch 62/100, loss: 1.166, accuracy: 1.000, time: 0.038742 seconds\n",
      "Epoch 63/100, loss: 1.166, accuracy: 1.000, time: 0.040226 seconds\n",
      "Epoch 64/100, loss: 1.166, accuracy: 1.000, time: 0.037589 seconds\n",
      "Epoch 65/100, loss: 1.166, accuracy: 1.000, time: 0.040051 seconds\n",
      "Epoch 66/100, loss: 1.166, accuracy: 1.000, time: 0.042406 seconds\n",
      "Epoch 67/100, loss: 1.166, accuracy: 1.000, time: 0.043936 seconds\n",
      "Epoch 68/100, loss: 1.166, accuracy: 1.000, time: 0.041268 seconds\n",
      "Epoch 69/100, loss: 1.166, accuracy: 1.000, time: 0.037173 seconds\n",
      "Epoch 70/100, loss: 1.166, accuracy: 1.000, time: 0.040287 seconds\n",
      "Epoch 71/100, loss: 1.166, accuracy: 1.000, time: 0.045109 seconds\n",
      "Epoch 72/100, loss: 1.166, accuracy: 1.000, time: 0.038039 seconds\n",
      "Epoch 73/100, loss: 1.166, accuracy: 1.000, time: 0.038045 seconds\n",
      "Epoch 74/100, loss: 1.166, accuracy: 1.000, time: 0.038153 seconds\n",
      "Epoch 75/100, loss: 1.166, accuracy: 1.000, time: 0.040997 seconds\n",
      "Epoch 76/100, loss: 1.166, accuracy: 1.000, time: 0.043854 seconds\n",
      "Epoch 77/100, loss: 1.166, accuracy: 1.000, time: 0.043727 seconds\n",
      "Epoch 78/100, loss: 1.166, accuracy: 1.000, time: 0.039215 seconds\n",
      "Epoch 79/100, loss: 1.166, accuracy: 1.000, time: 0.037692 seconds\n",
      "Epoch 80/100, loss: 1.166, accuracy: 1.000, time: 0.042003 seconds\n",
      "Epoch 81/100, loss: 1.166, accuracy: 1.000, time: 0.039608 seconds\n",
      "Epoch 82/100, loss: 1.166, accuracy: 1.000, time: 0.039423 seconds\n",
      "Epoch 83/100, loss: 1.166, accuracy: 1.000, time: 0.041415 seconds\n",
      "Epoch 84/100, loss: 1.166, accuracy: 1.000, time: 0.041501 seconds\n",
      "Epoch 85/100, loss: 1.166, accuracy: 1.000, time: 0.038262 seconds\n",
      "Epoch 86/100, loss: 1.166, accuracy: 1.000, time: 0.038667 seconds\n",
      "Epoch 87/100, loss: 1.166, accuracy: 1.000, time: 0.039218 seconds\n",
      "Epoch 88/100, loss: 1.166, accuracy: 1.000, time: 0.040780 seconds\n",
      "Epoch 89/100, loss: 1.166, accuracy: 1.000, time: 0.043982 seconds\n",
      "Epoch 90/100, loss: 1.166, accuracy: 1.000, time: 0.045507 seconds\n",
      "Epoch 91/100, loss: 1.166, accuracy: 1.000, time: 0.040186 seconds\n",
      "Epoch 92/100, loss: 1.166, accuracy: 1.000, time: 0.037299 seconds\n",
      "Epoch 93/100, loss: 1.166, accuracy: 1.000, time: 0.039959 seconds\n",
      "Epoch 94/100, loss: 1.165, accuracy: 1.000, time: 0.037181 seconds\n",
      "Epoch 95/100, loss: 1.165, accuracy: 1.000, time: 0.038794 seconds\n",
      "Epoch 96/100, loss: 1.165, accuracy: 1.000, time: 0.039558 seconds\n",
      "Epoch 97/100, loss: 1.165, accuracy: 1.000, time: 0.040089 seconds\n",
      "Epoch 98/100, loss: 1.165, accuracy: 1.000, time: 0.041155 seconds\n",
      "Epoch 99/100, loss: 1.165, accuracy: 1.000, time: 0.038511 seconds\n",
      "Epoch 100/100, loss: 1.165, accuracy: 1.000, time: 0.039647 seconds\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = results['epoch']\n",
    "train_loss = results['train_loss']\n",
    "plt.plot(epochs, train_loss)\n",
    "plt.title('Train loss (cross-entropy) per epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross-entropy loss')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_acc = results['val_acc']\n",
    "plt.plot(epochs, val_acc)\n",
    "plt.title('Validation accuracy per epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.782\n",
      "In the results illustrated above we see that the cross-entropy loss of the training set drops very quickly at the beginning\n",
      "Then, it stagnates and continues to drop very slowly over the remaining epochs\n",
      "\n",
      "In the plot of the validation accuracy per epoch, we see that the accuracy increases rapidly at the start and then stagnates for a number of epochs\n",
      "Although it is remarkable to see that the accuracy seems to have a rising trend starting from epoch 60, indicating that training for a longer time may be beneficial\n",
      "\n",
      "In the plot of the validation accuracy we see that the highest accuracy is around 0.76, which is close to the test accuracy of 0.782.\n",
      "This indicates that there's no symptoms of over fitting on the validation set\n"
     ]
    }
   ],
   "source": [
    "print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "print('In the results illustrated above we see that the cross-entropy loss of the training set drops very quickly at the beginning')\n",
    "print('Then, it stagnates and continues to drop very slowly over the remaining epochs\\n')\n",
    "\n",
    "print('In the plot of the validation accuracy per epoch, we see that the accuracy increases rapidly at the start and then stagnates for a number of epochs')\n",
    "print('Although it is remarkable to see that the accuracy seems to have a rising trend starting from epoch 60, indicating that training for a longer time may be beneficial\\n')\n",
    "\n",
    "print(f'In the plot of the validation accuracy we see that the highest accuracy is around 0.76, which is close to the test accuracy of {test_acc}.')\n",
    "print('This indicates that there\\'s no symptoms of over fitting on the validation set')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vb5YjHVClCqo",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Question 10 (0.5 pt)\n",
    "\n",
    "The paper introduces GCNs as a way to solve a *semi-supervised* classification problem.\n",
    "\n",
    "- What makes this problem semi-supervised?\n",
    "- What is the proportion of labeled data used for training with respect to labeled data in the validation and test sets? What is difference in this context with other benchmark tasks in machine learning, like image classification with MNIST?\n",
    "- Why do you think the GCN performs well in this semi-supervised scenario?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7qw58r1MmCUJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Your answer here\n",
    "print('This problem is semi-supervised because, labels are only available for a small subset of nodes.\\n')\n",
    "\n",
    "proportion = (data.train_mask.sum() / (data.val_mask.sum() + data.train_mask.sum())) * 100\n",
    "print(f'The proportion of labeled training data with respect to the labeled test and validation data is {proportion}%')\n",
    "print('The difference in this context with other benchmark tasks in machine learning is that the proportion of training instances is very small.')\n",
    "print('A typical data split utilizes around 60-70% of the entire labeled dataset for the training set\\n')\n",
    "\n",
    "print('By training the GCN model on the adjacency matrix which contains both labelled and unlabelled data, it causes the model to learn a larger graph representation, than it would if it only had access to labelled nodes.')\n",
    "print('Using this representation it can then infer information from it, allowing for better classification on unseen data.')\n"
   ],
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This problem is semi-supervised because, labels are only available for a small subset of nodes.\n",
      "\n",
      "The proportion of labeled training data with respect to the labeled test and validation data is 21.875%\n",
      "The difference in this context with other benchmark tasks in machine learning is that the proportion of training instances is very small.\n",
      "A typical data split utilizes around 60-70% of the entire labeled dataset for the training set\n",
      "\n",
      "By training the GCN model on the adjacency matrix which contains both labelled and unlabelled data, it causes the model to learn a larger graph representation, than it would if it only had access to labelled nodes.\n",
      "Using this representation it can then infer information from it, allowing for better classification on unseen data.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ihrjZddvz5d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading a dataset of proteins\n",
    "\n",
    "In the previous sections you learned how to pass the adjacency matrix of a graph with a couple of thousand of nodes, to classify each node with a particular label. A different and useful application of GCNs is graph classification.\n",
    "\n",
    "In contrast with the previous part, where there was a single, big graph, in graph classification we have multiple graphs, and each graph can be assigned a label. In this part of the assignment you will implement a classifier for proteins.\n",
    "\n",
    "[Proteins](https://en.wikipedia.org/wiki/Protein_(nutrient)) are parts of the buildings block of life. They consist of chains of amino acids, and can take many shapes. In the PROTEINS dataset, proteins are represented as graphs, where the nodes are amino acids, and an edge between them indicates that they are 6 [Angstroms](https://en.wikipedia.org/wiki/Angstrom) apart. All graphs have a binary label, where 1 means that the protein is not an enzyme.\n",
    "\n",
    "We will start by loading and examining this dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xmqweMcvnUH6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f10d56ce-4b1b-4925-bc61-58c49b22adb1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "dataset = TUDataset(root='data/TU', name='PROTEINS', use_node_attr=True)"
   ],
   "execution_count": 53,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oF1gyKPXiz-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Question 11 (0.25 pt)\n",
    "\n",
    "Unlike in the previous part, where we selected the first element returned by the loading function, note that here we get all the elements returned by `TUDataset()`. `dataset` is an interable object, that has some similar behaviors as a Python list: you can call `len()` on it, and you can takes slices from it.\n",
    "\n",
    "Each element in `dataset` is a `Data` object containing a graph that represents a protein. This is the same type of object that we used in the previous part to store the Cora citation network.\n",
    "\n",
    "Knowing this, answer the following:\n",
    "\n",
    "- How many proteins (graphs) are there in `dataset`?\n",
    "- Take any protein from `dataset`. How many nodes and edges does it contain? What is its label? How many features does each node have?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZNPsnXXbbHHe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Your answer here\n",
    "print(f'The dataset contains {len(dataset)} proteins')\n",
    "protein = dataset[0]\n",
    "print(f'The first protein of the dataset contains {protein.num_nodes} nodes and {protein.num_edges} edges.')\n",
    "print(f'Also, the proteins label is {protein.y.item()}, which means that the protein is an enzyme')\n",
    "print(f'Each node (protein) consists of {protein.num_features} features')"
   ],
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 1113 proteins\n",
      "The first protein of the dataset contains 42 nodes and 162 edges.\n",
      "Also, the proteins label is 0, which means that the protein is an enzyme\n",
      "Each node (protein) consists of 4 features\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHSklBZXpKpR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Question 12 (0.5 pt)\n",
    "\n",
    "To properly train and evaluate our model, we need training, validation, and test splits.\n",
    "\n",
    "For reproducibility purposes, we generate a random tensor of indices for you. Use it to extract the three splits from `dataset`.\n",
    "\n",
    "For training, take 80% of the indices (starting from the first element in `indices`), then the following 10% for validation, and the remaining 10% for testing. You can use the indices to index `dataset`.\n",
    "\n",
    "Call the resulting splits `train_dataset`, `valid_dataset`, and `test_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ttY4d1GInn08",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Don't erase the following three lines\n",
    "import torch\n",
    "torch.random.manual_seed(0)\n",
    "indices = torch.randperm(len(dataset))\n",
    "\n",
    "# Your answer here\n",
    "n_train = int(len(indices) * 0.8)\n",
    "n_validation = int(len(indices) * 0.1)\n",
    "\n",
    "indices_train = indices[:n_train]\n",
    "indices_val = indices[n_train:n_train + n_validation]\n",
    "indices_test = indices[n_train + n_validation:]\n",
    "\n",
    "train_dataset = dataset[indices_train]\n",
    "valid_dataset = dataset[indices_val]\n",
    "test_dataset = dataset[indices_test]\n",
    "\n",
    "print(f'This split resulted in the following no. instances for the train, validation and test set respectively: {len(train_dataset), len(valid_dataset), len(test_dataset)}')"
   ],
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This split resulted in the following no. instances for the train, validation and test set respectively: (890, 111, 112)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDJbB4CQqsfp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Working with a batch of graphs\n",
    "\n",
    "When working with the Cora dataset, you used the information in `data.edge_index` to build the sparse normalized adjacency matrix $\\hat{A}$ that is required by the GCN. We could do something similar here: for each graph, we build $\\hat{A}$, and pass it to the GCN. However, if the number of graphs is big, this can really slow down training.\n",
    "\n",
    "To avoid this, we will resort to a very useful trick that also allows us to reuse the same GCN you implemented previously. The trick makes it possible to do a forward pass through the GCN for multiple, disconnected graphs at the same time (instead of only one), much like when you train with mini-batches for other kinds of data.\n",
    "\n",
    "Let's first revisit the propagation rule of the GCN, $Z = \\hat{A}XW$, with an illustration (we have omitted the cells of $X$ and $W$ for clarity):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/02-gcn-forward.png\">\n",
    "\n",
    "If we have multiple graphs, we can still use the same propagation rule, if we\n",
    "\n",
    "- Set $\\hat{A}$ to be a block diagonal matrix, where the blocks are the different adjacency matrices of the graphs\n",
    "- Concatenate the feature matrices along the first dimension\n",
    "\n",
    "This is illustrated in the following figure, for a batch of 3 graphs. Note that the elements outside of the blocks are zero.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/02-gcn-batch-forward.png\">\n",
    "\n",
    "The resulting adjacency matrix $\\hat{A}_B$ can also be built as a sparse matrix, and once we have it together with the concatenated matrix of features, the computation of the graph convolution is exactly the same as before. Note how this trick also allows us to process graphs with different sizes and structures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DLPJ62b2mQ6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Question 13 (0.5 pt)\n",
    "\n",
    "\n",
    "Just as the citation network, the graphs in each of the datasets you created in Question 12 also have an `edge_index` attribute, which can be used to compute the normalized adjacency matrix $\\hat{A}$, for each graph.\n",
    "\n",
    "Reusing your code for Questions 3 and 5, define a function `get_a_norm()` that takes as input an element of a dataset (e.g. `train_dataset[0]`), and returns a `scipy.sparse` matrix containing $\\hat{A}$.\n",
    "\n",
    "Note that an element of a dataset has properties like `num_edges`, `num_nodes`, etc. which you can use here."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4nvPX2GB8oXp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Your answer here\n",
    "def get_a_norm(node):\n",
    "    # The rows and columns can be extracted from the edge_index of the dataset\n",
    "    row = node.edge_index[0]\n",
    "    col = node.edge_index[1]\n",
    "    num_edges = node.edge_index.shape[1]\n",
    "    # All the indices in the row and col vector are connected with each other, therefore we can generate a 1 (connection) for all of them\n",
    "    d = np.ones(num_edges, dtype=np.float32)\n",
    "    coo_mat = coo_matrix((d, (row, col)), shape=(node.num_nodes, node.num_nodes), dtype=np.float32)\n",
    "\n",
    "    a = coo_mat\n",
    "    a_tilde = a + identity(a.shape[0], dtype=np.float32)\n",
    "    diagonal = a_tilde.sum(axis=0, dtype=np.float32).A1\n",
    "    d_tilde = diags(diagonal, dtype=np.float32)\n",
    "    a_hat = d_tilde.power(-.5) @ a_tilde @ d_tilde.power(-.5)\n",
    "    return a_hat"
   ],
   "execution_count": 57,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can test this function by passing the first protein (node) of the dataset into it, resulting in a scipy.sparse matrix which contains A hat:\n",
      "  (0, 0)\t0.19999999\n",
      "  (0, 1)\t0.19999999\n",
      "  (0, 2)\t0.19999999\n",
      "  (0, 17)\t0.2236068\n",
      "  (0, 18)\t0.2236068\n",
      "  (1, 0)\t0.19999999\n",
      "  (1, 1)\t0.19999999\n",
      "  (1, 2)\t0.19999999\n",
      "  (1, 17)\t0.2236068\n",
      "  (1, 18)\t0.2236068\n",
      "  (2, 0)\t0.19999999\n",
      "  (2, 1)\t0.19999999\n",
      "  (2, 2)\t0.19999999\n",
      "  (2, 3)\t0.19999999\n",
      "  (2, 16)\t0.1825742\n",
      "  (3, 2)\t0.19999999\n",
      "  (3, 3)\t0.19999999\n",
      "  (3, 4)\t0.1825742\n",
      "  (3, 16)\t0.1825742\n",
      "  (3, 19)\t0.19999999\n",
      "  (4, 3)\t0.1825742\n",
      "  (4, 4)\t0.16666667\n",
      "  (4, 5)\t0.20412415\n",
      "  (4, 20)\t0.16666667\n",
      "  (4, 22)\t0.16666667\n",
      "  :\t:\n",
      "  (26, 13)\t0.1825742\n",
      "  (26, 26)\t0.16666667\n",
      "  (26, 27)\t0.1825742\n",
      "  (26, 29)\t0.16666667\n",
      "  (26, 30)\t0.20412415\n",
      "  (27, 12)\t0.2236068\n",
      "  (27, 26)\t0.1825742\n",
      "  (27, 27)\t0.19999999\n",
      "  (27, 28)\t0.19999999\n",
      "  (27, 29)\t0.1825742\n",
      "  (28, 10)\t0.2236068\n",
      "  (28, 11)\t0.2236068\n",
      "  (28, 12)\t0.2236068\n",
      "  (28, 27)\t0.19999999\n",
      "  (28, 28)\t0.19999999\n",
      "  (29, 13)\t0.1825742\n",
      "  (29, 14)\t0.20412415\n",
      "  (29, 26)\t0.16666667\n",
      "  (29, 27)\t0.1825742\n",
      "  (29, 29)\t0.16666667\n",
      "  (29, 30)\t0.20412415\n",
      "  (30, 15)\t0.25\n",
      "  (30, 26)\t0.20412415\n",
      "  (30, 29)\t0.20412415\n",
      "  (30, 30)\t0.25\n"
     ]
    }
   ],
   "source": [
    "print(f'We can test this function by passing the first protein (node) of the dataset into it, resulting in a scipy.sparse matrix which contains A hat:\\n{get_a_norm(train_dataset[0])}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBrmYBY3AfhW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Question 14 (1 pt)\n",
    "\n",
    "To prepare the batch of graphs, we need to collect multiple adjacency matrices, feature matrices, and labels.\n",
    "\n",
    "When using the trick described in the last figure, we see that we have to keep track of when a graph starts and when it ends, so that we can later differentiate the outputs due to $X^{(0)}$, $X^{(1)}$, etc. To achieve this, we will additionally collect a 1D array of batch indices, one for each $X^{(i)}$.\n",
    "\n",
    "The 1D array has as many elements as rows in $X^{(i)}$, and it is filled with the value $i$ (the position of $X^{(i)}$ in the batch):\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/03-batch-indices.png\">\n",
    "\n",
    "We will later concatenate all the 1D arrays along the first dimension, just as we will do with all the $X^{(i)}$.\n",
    "\n",
    "Define a function `prepare_graphs_batch()` that takes as input a dataset (e.g. `train_dataset`), and does the following\n",
    "\n",
    "- Create four empty lists:\n",
    "  - `adj_matrices`\n",
    "  - `feature_matrices`\n",
    "  - `batch_indices`\n",
    "  - `labels`\n",
    "- Iterate over the input dataset, getting one graph at a time. At each step, use your function from Question 13 to append the adjacency matrix to `adj_matrices`, append the matrix of input features to `feature_matrices`, create the array of batch indices (as explained above) and append it to `batch_indices`, and append the label of the graph to `labels`. **Make sure to convert the label to float**.\n",
    "- Once the loop is over, use `scipy.sparse.block_diag()` to build the block diagonal matrix $\\hat{A}_B$. Convert it to the COO format, and then use your answer to Question 6 to turn it into a sparse PyTorch tensor.\n",
    "- Use `torch.cat()` to concatenate the tensors in `feature_matrices` along the first dimension. Do this also for `batch_indices` and `labels`.\n",
    "- Return the 4 tensors computed in the previous two items."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SsQ0-JjSqFgD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from scipy.sparse import block_diag\n",
    "\n",
    "# Your answer here\n",
    "def prepare_graphs_batch(dataset):\n",
    "    adj_matrices, feature_matrices, batch_indices, labels = [], [], [], []\n",
    "    tot_nodes = 0\n",
    "\n",
    "    for i, graph in enumerate(dataset):\n",
    "        adj_matrix = get_a_norm(node=graph)\n",
    "        adj_matrices.append(adj_matrix)\n",
    "        feature_matrices.append(graph.x)\n",
    "        batch_index = torch.tensor([i]).repeat(graph.num_nodes)\n",
    "        tot_nodes += graph.num_nodes\n",
    "        batch_indices.append(batch_index)\n",
    "        labels.append(graph.y.to(torch.float))\n",
    "    A_hat_block = block_diag(feature_matrices)\n",
    "\n",
    "    indices = torch.tensor([np.arange(A_hat_block.shape[1]), np.arange(A_hat_block.shape[1])])\n",
    "    values = torch.tensor(np.array(np.diagonal(A_hat_block.toarray())))\n",
    "    sparse_coo = sparse_coo_tensor(indices, values, dtype=torch.float)\n",
    "    feature_matrices_cat = torch.cat(feature_matrices, dim=0)\n",
    "    batch_indices_cat = torch.cat(batch_indices, dim=0)\n",
    "    labels_cat = torch.cat(labels, dim=0)\n",
    "    return sparse_coo, feature_matrices_cat, batch_indices_cat, labels_cat\n",
    "\n"
   ],
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor(indices=tensor([[   0,    1,    2,  ..., 3557, 3558, 3559],\n                        [   0,    1,    2,  ..., 3557, 3558, 3559]]),\n        values=tensor([12.,  1.,  0.,  ...,  0.,  0.,  0.]),\n        size=(3560, 3560), nnz=3560, layout=torch.sparse_coo),\n tensor([[12.,  1.,  0.,  0.],\n         [10.,  1.,  0.,  0.],\n         [11.,  1.,  0.,  0.],\n         ...,\n         [ 4.,  0.,  1.,  0.],\n         [ 6.,  0.,  1.,  0.],\n         [ 6.,  0.,  1.,  0.]]),\n tensor([  0,   0,   0,  ..., 889, 889, 889]),\n tensor([0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n         1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0.,\n         1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1.,\n         1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0.,\n         0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n         1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n         1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n         0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0.,\n         0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n         0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0.,\n         1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n         0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n         0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n         1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n         0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n         0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1.,\n         1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n         0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n         0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n         1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n         1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n         0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n         1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n         0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n         1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n         0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n         0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n         0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n         1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n         1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n         1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n         0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n         1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0.,\n         1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n         0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n         0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n         1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n         0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,\n         0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1.,\n         0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n         1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n         0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0.,\n         1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0.,\n         0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.,\n         0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n         0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n         1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n         0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0.,\n         0., 0., 1., 1., 0., 1., 0., 1.]))"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_graphs_batch(dataset=train_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i73P_EU0MSPX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once your answer for the previous question is ready, you can run the next cell to prepare all the required information, for the train, validation, and test splits."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Iol5FxJGMmAU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "train_a_norm, train_features, train_batch_idx, train_labels = prepare_graphs_batch(train_dataset)\n",
    "valid_a_norm, valid_features, valid_batch_idx, valid_labels = prepare_graphs_batch(valid_dataset)\n",
    "test_a_norm, test_features, test_batch_idx, test_labels = prepare_graphs_batch(test_dataset)"
   ],
   "execution_count": 75,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    device_name = 'cpu'\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device_name = 'cuda:0'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device_name = 'mps'\n",
    "    device = torch.device(device_name)\n",
    "    print(f\"Using device: {device_name}\")\n",
    "    return device\n",
    "device = get_device()\n",
    "\n",
    "# train_features, train_batch_idx, train_labels = train_features.to(device), train_batch_idx.to(device), train_labels.to(device)\n",
    "# valid_features, valid_batch_idx, valid_labels = valid_features.to(device), valid_batch_idx.to(device), valid_labels.to(device)\n",
    "# test_features, test_batch_idx, test_labels = test_features.to(device), test_batch_idx.to(device), test_labels.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6q-JU87NClh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### GCNs for graph classification\n",
    "\n",
    "We now have all the ingredients to pass a batch of graphs to a GCN. However, for each graph in the batch, the output $Z^{(i)}$ contains one row for each node in the graph. If the goal is to do classification at the graph level, we have to *pool* these vectors to then compute the required logits for classification.\n",
    "\n",
    "This operation is similar as how pooling works in a CNN. We could consider taking the mean of the vectors, the sum, or use max-pooling. The difference with respect to CNNs is that in our case, we have a batch of graphs, each potentially with a different number of nodes.\n",
    "\n",
    "To implement this specific pooling, we can use the scatter operation in the `torch_scatter` library, which comes when installing PyG. We will use it, together with the tensor of batch indices from the previous two questions, to pool the outputs of the GCN for each graph, into a single vector:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dfdazac/dlvu-a5/main/img/04-scatter.png\">\n",
    "\n",
    "You can check more details in the [documentation](https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "750WraywwYDH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Your answer here\n",
    "class GraphClassifier(torch.nn.Module):\n",
    "    def __init__(self, n_input_features, out_dim_hid, n_classes):\n",
    "        super().__init__()\n",
    "        self.gcn = GCN(n_input_features=n_input_features, out_dim_hid=out_dim_hid, out_dim_out=n_classes)\n",
    "        self.linear = torch.nn.Linear(in_features=out_dim_hid, out_features=n_classes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, X, sparse_coo, batch_indices):\n",
    "        X = self.gcn(X, sparse_coo)\n",
    "        X = self.relu(X)\n",
    "        destination = torch.zeros((len(torch.unique(batch_indices)), X.shape[1]))\n",
    "        scatter = destination.scatter(dim=0, index=batch_indices, src=X)\n",
    "        glob_max_pool = scatter.max(dim=0)\n",
    "        return self.linear(glob_max_pool)\n"
   ],
   "execution_count": 77,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0QHnn6dV87J",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1PHy-_vTjgh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Question 16 (1.5 pt)\n",
    "\n",
    "Implement a training loop for the graph classifier. Use the data from Question 14 to train and evaluate the model.\n",
    "\n",
    "We encourage you to use a GPU in this section for faster training. Note that if you change the runtime at this point, you must re-execute several of the cells above, including the ones that install PyG.\n",
    "\n",
    "- Instantiate a classifier with 32 as the hidden dimension\n",
    "- Use Adam with a learning rate of 1e-3.\n",
    "- Use `torch.nn.BCEWithLogitsLoss` as the loss function.\n",
    "- Train for 5,000 epochs. Once training is done, plot the loss curve and the accuracy in the validation set. Then report the accuracy in the test set.\n",
    "\n",
    "**Note:** the logits from the output of the classifier come from a linear layer. To compute actual predictions for the calculation of the accuracy, pass the logits through `torch.sigmoid()`, and set the predicted values to 1 whenever they are greater than 0.5, and to 0 otherwise.\n",
    "\n",
    "You should get an accuracy equal to or higher than 70% in the validation and test sets. Can you beat the [state-of-the-art](https://paperswithcode.com/sota/graph-classification-on-proteins)? Feel free to modify your architecture and experiment with it.\n",
    "\n",
    "Discuss what you observe during training and your results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9DbGAs8W2Xja",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# If your runtime is GPU-enabled, use .to(device) to move the model\n",
    "# and all the relevant tensors to the GPU. You have to move tensors back to CPU\n",
    "# when computing metrics like accuracy, using .cpu().\n",
    "\n",
    "# Your answer here\n",
    "# We set n_classes to 1, because we need to assign the classes based on sigmoid(y) > 0.5 -> class A, otherwise class B\n",
    "graph_classifier = GraphClassifier(n_input_features=dataset.num_features, out_dim_hid=32, n_classes=1)\n",
    "optimizer = torch.optim.Adam(params=graph_classifier.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "epochs = 100\n",
    "\n",
    "# def evaluate():\n",
    "\n",
    "def training_loop():\n",
    "    train_n_instances = len(train_a_norm)\n",
    "    results = {\n",
    "        'epoch': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    graph_classifier.train()\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = graph_classifier(train_features, train_a_norm, train_batch_idx)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        predicted_labels = torch.tensor([1 if output > .5 else 0 for output in outputs])\n",
    "        n_correct_train = (predicted_labels == train_labels).sum().item()\n",
    "        loss = criterion(predicted_labels, train_labels)\n",
    "        running_loss_train = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        results['epoch'].append(epoch + 1)\n",
    "        results['val_loss'].append(running_loss_train)\n",
    "        results['val_acc'].append(evaluate(A_hat=A_hat, data=data))\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, loss: {(running_loss_train):.3f}, accuracy: {(n_correct_train / train_n_instances):.3f}, time: {(time.time() - start):2f} seconds')\n",
    "    return model, results\n",
    "\n",
    "model, results = training_loop()\n"
   ],
   "execution_count": 83,
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::sparse_dim' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_dim' is only available for these backends: [MPS, SparseCPU, SparseMeta, SparseCsrCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:39 [backend fallback]\nSparseCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseCPU.cpp:1261 [kernel]\nSparseMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseMeta.cpp:249 [kernel]\nSparseCsrCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseCsrCPU.cpp:1030 [kernel]\nBackendSelect: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradCUDA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradHIP: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradXLA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradIPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradXPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradHPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradVE: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradLazy: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradPrivateUse1: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradPrivateUse2: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradPrivateUse3: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nTracer: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[83], line 41\u001B[0m\n\u001B[1;32m     38\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m(running_loss_train)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m(n_correct_train \u001B[38;5;241m/\u001B[39m train_n_instances)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.3f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, time: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m(time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m seconds\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model, results\n\u001B[0;32m---> 41\u001B[0m model, results \u001B[38;5;241m=\u001B[39m \u001B[43mtraining_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[83], line 26\u001B[0m, in \u001B[0;36mtraining_loop\u001B[0;34m()\u001B[0m\n\u001B[1;32m     23\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     25\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 26\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mgraph_classifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_a_norm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_batch_idx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msigmoid(outputs)\n\u001B[1;32m     28\u001B[0m predicted_labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m output \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m.5\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m outputs])\n",
      "File \u001B[0;32m/Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[77], line 10\u001B[0m, in \u001B[0;36mGraphClassifier.forward\u001B[0;34m(self, X, sparse_coo, batch_indices)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, sparse_coo, batch_indices):\n\u001B[0;32m---> 10\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgcn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse_coo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(X)\n\u001B[1;32m     12\u001B[0m     destination \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros((\u001B[38;5;28mlen\u001B[39m(torch\u001B[38;5;241m.\u001B[39munique(batch_indices)), X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]))\n",
      "File \u001B[0;32m/Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[47], line 13\u001B[0m, in \u001B[0;36mGCN.forward\u001B[0;34m(self, X, A_hat)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, A_hat):\n\u001B[0;32m---> 13\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhidden\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mA_hat\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(X)\n\u001B[1;32m     15\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(X, A_hat)\n",
      "File \u001B[0;32m/Volumes/Transcend/code/Deep-Learning-VU-Group-Assignments/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[46], line 9\u001B[0m, in \u001B[0;36mGCNLayer.forward\u001B[0;34m(self, X, A_hat)\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, A_hat):\n\u001B[0;32m----> 9\u001B[0m     Z \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mspmm(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspmm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mA_hat\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mT, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mW)\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Z\n",
      "\u001B[0;31mNotImplementedError\u001B[0m: Could not run 'aten::sparse_dim' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::sparse_dim' is only available for these backends: [MPS, SparseCPU, SparseMeta, SparseCsrCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:39 [backend fallback]\nSparseCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseCPU.cpp:1261 [kernel]\nSparseMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseMeta.cpp:249 [kernel]\nSparseCsrCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/RegisterSparseCsrCPU.cpp:1030 [kernel]\nBackendSelect: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:140 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:488 [backend fallback]\nFunctionalize: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:291 [backend fallback]\nNamed: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\nNegative: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\nAutogradOther: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradCPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradCUDA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradHIP: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradXLA: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradMPS: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradIPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradXPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradHPU: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradVE: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradLazy: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradMeta: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradPrivateUse1: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradPrivateUse2: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradPrivateUse3: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nAutogradNestedTensor: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/VariableType_4.cpp:14636 [autograd kernel]\nTracer: registered at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/autograd/generated/TraceType_4.cpp:12871 [kernel]\nAutocastCPU: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:482 [backend fallback]\nAutocastCUDA: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:324 [backend fallback]\nFuncTorchBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:743 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/BatchingRegistrations.cpp:1064 [backend fallback]\nVmapMode: fallthrough registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:189 [backend fallback]\nPythonTLSSnapshot: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:484 [backend fallback]\nPythonDispatcher: registered at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(indices=tensor([[   0,    1,    2,  ..., 3557, 3558, 3559],\n                       [   0,    1,    2,  ..., 3557, 3558, 3559]]),\n       values=tensor([12.,  1.,  0.,  ...,  0.,  0.,  0.]),\n       size=(3560, 3560), nnz=3560, layout=torch.sparse_coo)"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_a_norm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7QhyAMms8-L",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Grading (10pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Question 1: 0.25pt \n",
    "- Question 2: 0.25pt \n",
    "- Question 3: 0.5pt \n",
    "- Question 4: 0.25pt \n",
    "- Question 5: 0.5pt \n",
    "- Question 6: 0.5pt \n",
    "- Question 7: 0.5pt \n",
    "- Question 8: 0.5pt \n",
    "- Question 9: 1.5pt \n",
    "- Question 10: 0.5pt \n",
    "- Question 11: 0.25pt \n",
    "- Question 12: 0.5pt \n",
    "- Question 13: 0.5pt \n",
    "- Question 14: 1pt\n",
    "- Question 15: 1pt\n",
    "- Question 16: 1.5pt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}